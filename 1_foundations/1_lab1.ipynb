{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the start of your adventure in Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Are you ready for action??</h2>\n",
    "            <span style=\"color:#ff7800;\">Have you completed all the setup steps in the <a href=\"../setup/\">setup</a> folder?<br/>\n",
    "            Have you read the <a href=\"../README.md\">README</a>? Many common questions are answered here!<br/>\n",
    "            Have you checked out the guides in the <a href=\"../guides/01_intro.ipynb\">guides</a> folder?<br/>\n",
    "            Well in that case, you're ready!!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">This code is a live resource - keep an eye out for my updates</h2>\n",
    "            <span style=\"color:#00bfff;\">I push updates regularly. As people ask questions or have problems, I add more examples and improve explanations. As a result, the code below might not be identical to the videos, as I've added more steps and better comments. Consider this like an interactive book that accompanies the lectures.<br/><br/>\n",
    "            I try to send emails regularly with important updates related to the course. You can find this in the 'Announcements' section of Udemy in the left sidebar. You can also choose to receive my emails via your Notification Settings in Udemy. I'm respectful of your inbox and always try to add value with my emails!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And please do remember to contact me if I can help\n",
    "\n",
    "And I love to connect: https://www.linkedin.com/in/eddonner/\n",
    "\n",
    "\n",
    "### New to Notebooks like this one? Head over to the guides folder!\n",
    "\n",
    "Just to check you've already added the Python and Jupyter extensions to Cursor, if not already installed:\n",
    "- Open extensions (View >> extensions)\n",
    "- Search for python, and when the results show, click on the ms-python one, and Install it if not already installed\n",
    "- Search for jupyter, and when the results show, click on the Microsoft one, and Install it if not already installed  \n",
    "Then View >> Explorer to bring back the File Explorer.\n",
    "\n",
    "And then:\n",
    "1. Click where it says \"Select Kernel\" near the top right, and select the option called `.venv (Python 3.12.9)` or similar, which should be the first choice or the most prominent choice. You may need to choose \"Python Environments\" first.\n",
    "2. Click in each \"cell\" below, starting with the cell immediately below this text, and press Shift+Enter to run\n",
    "3. Enjoy!\n",
    "\n",
    "After you click \"Select Kernel\", if there is no option like `.venv (Python 3.12.9)` then please do the following:  \n",
    "1. On Mac: From the Cursor menu, choose Settings >> VS Code Settings (NOTE: be sure to select `VSCode Settings` not `Cursor Settings`);  \n",
    "On Windows PC: From the File menu, choose Preferences >> VS Code Settings(NOTE: be sure to select `VSCode Settings` not `Cursor Settings`)  \n",
    "2. In the Settings search bar, type \"venv\"  \n",
    "3. In the field \"Path to folder with a list of Virtual Environments\" put the path to the project root, like C:\\Users\\username\\projects\\agents (on a Windows PC) or /Users/username/projects/agents (on Mac or Linux).  \n",
    "And then try again.\n",
    "\n",
    "Having problems with missing Python versions in that list? Have you ever used Anaconda before? It might be interferring. Quit Cursor, bring up a new command line, and make sure that your Anaconda environment is deactivated:    \n",
    "`conda deactivate`  \n",
    "And if you still have any problems with conda and python versions, it's possible that you will need to run this too:  \n",
    "`conda config --set auto_activate_base false`  \n",
    "and then from within the Agents directory, you should be able to run `uv python list` and see the Python 3.12 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's do an import. If you get an Import Error, double check that your Kernel is correct..\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next it's time to load the API keys into environment variables\n",
    "# If this returns false, see the next cell!\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, did that just output `False`??\n",
    "\n",
    "If so, the most common reason is that you didn't save your `.env` file after adding the key! Be sure to have saved.\n",
    "\n",
    "Also, make sure the `.env` file is named precisely `.env` and is in the project root directory (`agents`)\n",
    "\n",
    "By the way, your `.env` file should have a stop symbol next to it in Cursor on the left, and that's actually a good thing: that's Cursor saying to you, \"hey, I realize this is a file filled with secret information, and I'm not going to send it to an external AI to suggest changes, because your keys should not be shown to anyone else.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Final reminders</h2>\n",
    "            <span style=\"color:#ff7800;\">1. If you're not confident about Environment Variables or Web Endpoints / APIs, please read Topics 3 and 5 in this <a href=\"../guides/04_technical_foundations.ipynb\">technical foundations guide</a>.<br/>\n",
    "            2. If you want to use AIs other than OpenAI, like Gemini, DeepSeek or Ollama (free), please see the first section in this <a href=\"../guides/09_ai_apis_and_ollama.ipynb\">AI APIs guide</a>.<br/>\n",
    "            3. If you ever get a Name Error in Python, you can always fix it immediately; see the last section of this <a href=\"../guides/06_python_foundations.ipynb\">Python Foundations guide</a> and follow both tutorials and exercises.<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Check the key - if you're not using OpenAI, check whichever key you're using! Ollama doesn't need a key.\n",
    "\n",
    "import os\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set - please head to the troubleshooting guide in the setup folder\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - the all important import statement\n",
    "# If you get an import error - head over to troubleshooting in the Setup folder\n",
    "# Even for other LLM providers like Gemini, you still use this OpenAI import - see Guide 9 for why\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we'll create an instance of the OpenAI class\n",
    "# If you're not sure what it means to create an instance of a class - head over to the guides folder (guide 6)!\n",
    "# If you get a NameError - head over to the guides folder (guide 6)to learn about NameErrors - always instantly fixable\n",
    "# If you're not using OpenAI, you just need to slightly modify this - precise instructions are in the AI APIs guide (guide 9)\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of messages in the familiar OpenAI format\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "# And now call it! Any problems, head to the troubleshooting guide\n",
    "# This uses GPT 4.1 nano, the incredibly cheap model\n",
    "# The APIs guide (guide 9) has exact instructions for using even cheaper or free alternatives to OpenAI\n",
    "# If you get a NameError, head to the guides folder (guide 6) to learn about NameErrors - always instantly fixable\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - let's ask for a question:\n",
    "\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If all Bloops are Razzies and all Razzies are Lazzies, which of the following must be true?\n",
      "\n",
      "A) All Bloops are definitely Lazzies.  \n",
      "B) Some Lazzies are Bloops.  \n",
      "C) No Bloop is a Lazzie.  \n",
      "D) Some Razzies are not Bloops.  \n",
      "\n",
      "Explain your reasoning.\n"
     ]
    }
   ],
   "source": [
    "# ask it - this uses GPT 4.1 mini, still cheap but more powerful than nano\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a new messages list\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze the given statements step-by-step:\n",
      "\n",
      "**Given:**  \n",
      "1. All Bloops are Razzies.  \n",
      "2. All Razzies are Lazzies.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Understanding the relationships\n",
      "\n",
      "- **All Bloops are Razzies** means every Bloop is included inside the set of Razzies.\n",
      "  \n",
      "  Symbolically:  \n",
      "  \\( \\text{Bloop} \\subseteq \\text{Razzie} \\)\n",
      "\n",
      "- **All Razzies are Lazzies** means every Razzie is included inside the set of Lazzies.\n",
      "  \n",
      "  Symbolically:  \n",
      "  \\( \\text{Razzie} \\subseteq \\text{Lazzie} \\)\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Logical conclusion by transitivity\n",
      "\n",
      "Since all Bloops are Razzies, and all Razzies are Lazzies, it follows:\n",
      "\n",
      "\\[\n",
      "\\text{Bloop} \\subseteq \\text{Razzie} \\subseteq \\text{Lazzie}\n",
      "\\]\n",
      "\n",
      "Therefore:\n",
      "\n",
      "\\[\n",
      "\\text{Bloop} \\subseteq \\text{Lazzie}\n",
      "\\]\n",
      "\n",
      "Meaning:\n",
      "\n",
      "- **All Bloops are Lazzies.**\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Evaluate each option\n",
      "\n",
      "- **A) All Bloops are definitely Lazzies.**  \n",
      "  This is **true**, as shown.\n",
      "\n",
      "- **B) Some Lazzies are Bloops.**  \n",
      "  If all Bloops are Lazzies, then necessarily *some* Lazzies (at least those Bloops) exist.  \n",
      "  However, this requires there to be at least one Bloop (non-empty set). Since the problem does not specify existence, \"some\" statements can be uncertain.\n",
      "\n",
      "- **C) No Bloop is a Lazzie.**  \n",
      "  This is **false** because we've shown all Bloops are Lazzies.\n",
      "\n",
      "- **D) Some Razzies are not Bloops.**  \n",
      "  Not necessarily true. It is possible that all Razzies are Bloops (though not specified), or some are not. The statement does not **must** be true.\n",
      "\n",
      "---\n",
      "\n",
      "### Final conclusion:\n",
      "\n",
      "**The statement that must be true based on given information is:**\n",
      "\n",
      "**A) All Bloops are definitely Lazzies.**\n",
      "\n",
      "---\n",
      "\n",
      "# Summary:\n",
      "\n",
      "- Because all Bloops are contained within Razzies, and all Razzies within Lazzies, all Bloops must be Lazzies.\n",
      "- Without additional information about the existence or number of Bloops, statements about \"some\" or \"no\" cannot be guaranteed.\n",
      "- Hence, **only A is necessarily true**.\n"
     ]
    }
   ],
   "source": [
    "# Ask it again\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the given statements step-by-step:\n",
       "\n",
       "**Given:**  \n",
       "1. All Bloops are Razzies.  \n",
       "2. All Razzies are Lazzies.\n",
       "\n",
       "---\n",
       "\n",
       "### Step 1: Understanding the relationships\n",
       "\n",
       "- **All Bloops are Razzies** means every Bloop is included inside the set of Razzies.\n",
       "  \n",
       "  Symbolically:  \n",
       "  \\( \\text{Bloop} \\subseteq \\text{Razzie} \\)\n",
       "\n",
       "- **All Razzies are Lazzies** means every Razzie is included inside the set of Lazzies.\n",
       "  \n",
       "  Symbolically:  \n",
       "  \\( \\text{Razzie} \\subseteq \\text{Lazzie} \\)\n",
       "\n",
       "---\n",
       "\n",
       "### Step 2: Logical conclusion by transitivity\n",
       "\n",
       "Since all Bloops are Razzies, and all Razzies are Lazzies, it follows:\n",
       "\n",
       "\\[\n",
       "\\text{Bloop} \\subseteq \\text{Razzie} \\subseteq \\text{Lazzie}\n",
       "\\]\n",
       "\n",
       "Therefore:\n",
       "\n",
       "\\[\n",
       "\\text{Bloop} \\subseteq \\text{Lazzie}\n",
       "\\]\n",
       "\n",
       "Meaning:\n",
       "\n",
       "- **All Bloops are Lazzies.**\n",
       "\n",
       "---\n",
       "\n",
       "### Step 3: Evaluate each option\n",
       "\n",
       "- **A) All Bloops are definitely Lazzies.**  \n",
       "  This is **true**, as shown.\n",
       "\n",
       "- **B) Some Lazzies are Bloops.**  \n",
       "  If all Bloops are Lazzies, then necessarily *some* Lazzies (at least those Bloops) exist.  \n",
       "  However, this requires there to be at least one Bloop (non-empty set). Since the problem does not specify existence, \"some\" statements can be uncertain.\n",
       "\n",
       "- **C) No Bloop is a Lazzie.**  \n",
       "  This is **false** because we've shown all Bloops are Lazzies.\n",
       "\n",
       "- **D) Some Razzies are not Bloops.**  \n",
       "  Not necessarily true. It is possible that all Razzies are Bloops (though not specified), or some are not. The statement does not **must** be true.\n",
       "\n",
       "---\n",
       "\n",
       "### Final conclusion:\n",
       "\n",
       "**The statement that must be true based on given information is:**\n",
       "\n",
       "**A) All Bloops are definitely Lazzies.**\n",
       "\n",
       "---\n",
       "\n",
       "# Summary:\n",
       "\n",
       "- Because all Bloops are contained within Razzies, and all Razzies within Lazzies, all Bloops must be Lazzies.\n",
       "- Without additional information about the existence or number of Bloops, statements about \"some\" or \"no\" cannot be guaranteed.\n",
       "- Hence, **only A is necessarily true**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(answer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "That was a small, simple step in the direction of Agentic AI, with your new environment!\n",
    "\n",
    "Next time things get more interesting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Now try this commercial application:<br/>\n",
    "            First ask the LLM to pick a business area that might be worth exploring for an Agentic AI opportunity.<br/>\n",
    "            Then ask the LLM to present a pain-point in that industry - something challenging that might be ripe for an Agentic solution.<br/>\n",
    "            Finally have 3 third LLM call propose the Agentic AI solution. <br/>\n",
    "            We will cover this at up-coming labs, so don't worry if you're unsure.. just give it a try!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an excellent and practical approach to bridging LLMs with robotics! To build on your overview and proposed example, here’s a concrete solution to streamline and enhance the workflow of leveraging an LLM for command parsing and robotic action execution:\n",
      "\n",
      "---\n",
      "\n",
      "## Proposed Solution: Modular LLM-Powered Robotics Command Pipeline\n",
      "\n",
      "### Key Idea\n",
      "\n",
      "Create a modular software pipeline integrating natural language command parsing (using an LLM), grounding in perception modules, and robot control, with clear interfaces and fallback/feedback loops.\n",
      "\n",
      "---\n",
      "\n",
      "### Architecture Overview\n",
      "\n",
      "```\n",
      "User Command (NL)  -->  LLM Parser  -->  Structured Action Plan (JSON)  -->  \n",
      "     Perception Module (Vision, Localization) --> Plan Refinement & Validation -->  \n",
      "     Robot Control Interface  -->  Robot Execution  \n",
      "         ^                                                         |  \n",
      "         |------------------------------------ Feedback / Clarification Loop  \n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Detailed Pipeline Steps & Enhancements\n",
      "\n",
      "| Step                                      | Enhancement                                                 | Tools / Tips                              |\n",
      "|-------------------------------------------|-------------------------------------------------------------|------------------------------------------|\n",
      "| **1. Language Parsing (Your example)**    | - Use **few-shot prompting** or fine-tune an LLM on robotics task commands;<br>- Probabilistic parsing to output confidence scores | OpenAI GPT API / Local LLM (Llama, etc.) |\n",
      "| **2. Perception & Grounding**              | - Use vision models (e.g., CLIP or BLIP) to identify referenced objects in the scene <br>- Map language references (“red cup”) to detected objects with IDs | OpenCV, PyTorch + pre-trained vision models, ROS perception stack        |\n",
      "| **3. Plan Refinement & Validation**        | - Check whether the planned actions are feasible in the current environment<br>- Validate object locations, accessibility, and motion constraints | Use physics simulator (PyBullet, Isaac Gym), implement collision checks  |\n",
      "| **4. Execution Interface (ROS integration)**| - Translate JSON action steps into appropriate ROS messages or robot-specific API calls                        | ROS action servers, MoveIt! for motion planning                          |\n",
      "| **5. Feedback Loop**                        | - Robot confirms plan or requests clarification<br>- Integrate multi-turn dialogues with the LLM                     | LangChain for multi-step interactions, Dialogflow, or custom logic       |\n",
      "\n",
      "---\n",
      "\n",
      "### Sample Extended Python Snippet Sketch\n",
      "\n",
      "Here’s a sketch that extends your example with feedback and grounding placeholders:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "\n",
      "openai.api_key = \"YOUR_API_KEY\"\n",
      "\n",
      "def parse_command(command_text):\n",
      "    prompt = f\"\"\"\n",
      "    You are an assistant that converts natural language robot commands into structured steps.\n",
      "    Input command: \"{command_text}\"\n",
      "    Output steps as JSON list of simple actions with parameters, e.g.:\n",
      "    [\n",
      "        {{\"action\": \"move\", \"object\": \"red cup\", \"from\": \"table\"}},\n",
      "        {{\"action\": \"place\", \"object\": \"red cup\", \"location\": \"sink\"}}\n",
      "    ]\n",
      "    \"\"\"\n",
      "    response = openai.ChatCompletion.create(\n",
      "        model=\"gpt-4\",\n",
      "        messages=[\n",
      "            {\"role\": \"system\", \"content\": \"You convert commands to robot action plans.\"},\n",
      "            {\"role\": \"user\", \"content\": prompt}\n",
      "        ],\n",
      "        temperature=0,\n",
      "        max_tokens=300,\n",
      "    )\n",
      "    steps_json = response['choices'][0]['message']['content']\n",
      "    return steps_json\n",
      "\n",
      "def ground_objects(steps, perception_data):\n",
      "    # Example: Map \"red cup\" -> detected object ID from perception\n",
      "    grounded_steps = []\n",
      "    for step in steps:\n",
      "        obj_name = step.get(\"object\", None)\n",
      "        if obj_name and obj_name in perception_data:\n",
      "            step[\"object_id\"] = perception_data[obj_name][\"id\"]\n",
      "        grounded_steps.append(step)\n",
      "    return grounded_steps\n",
      "\n",
      "def validate_plan(steps):\n",
      "    # Placeholder for plan plausibility checks (collision, reachability)\n",
      "    # Return True if valid, else False\n",
      "    return True\n",
      "\n",
      "def execute_plan(steps, robot_interface):\n",
      "    for step in steps:\n",
      "        # Convert step to robot commands\n",
      "        robot_interface.send_command(step)\n",
      "\n",
      "def main(command_text):\n",
      "    raw_plan = parse_command(command_text)\n",
      "    print(\"Raw plan:\", raw_plan)\n",
      "\n",
      "    # Convert raw_plan string to JSON (handle errors!)\n",
      "    import json\n",
      "    try:\n",
      "        steps = json.loads(raw_plan)\n",
      "    except json.JSONDecodeError:\n",
      "        print(\"Failed to parse plan JSON.\")\n",
      "        return\n",
      "\n",
      "    # Perception data mockup\n",
      "    perception_data = {\n",
      "        \"red cup\": {\"id\": 101, \"location\": \"table\"},\n",
      "        \"sink\": {\"id\": 202, \"location\": \"sink_area\"}\n",
      "    }\n",
      "\n",
      "    grounded_steps = ground_objects(steps, perception_data)\n",
      "    if not validate_plan(grounded_steps):\n",
      "        print(\"Plan validation failed: plan not executable.\")\n",
      "        return\n",
      "\n",
      "    # Mock robot interface\n",
      "    class RobotInterface:\n",
      "        def send_command(self, cmd):\n",
      "            print(f\"Executing: {cmd}\")\n",
      "\n",
      "    robot_interface = RobotInterface()\n",
      "    execute_plan(grounded_steps, robot_interface)\n",
      "    print(\"Plan executed successfully.\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    user_command = \"Pick up the red cup from the table and put it in the sink.\"\n",
      "    main(user_command)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Summary\n",
      "\n",
      "- **LLM parses commands → structured plans**\n",
      "- **Perception grounds language references → real objects**\n",
      "- **Validation layer ensures feasibility**\n",
      "- **Robot interface executes validated plan**\n",
      "- **Feedback enables clarification and safety**\n",
      "\n",
      "This modular approach lets you gradually improve each stage while testing in simulation or on real hardware.\n",
      "\n",
      "---\n",
      "\n",
      "If you want, I can also share:\n",
      "\n",
      "- Sample ROS node templates bridging LLM output with robot motion planning\n",
      "- Vision-language grounding demo codes\n",
      "- Multi-turn clarification dialogs with LangChain\n",
      "- Dataset preprocessing scripts for ALFRED or TEACh\n",
      "\n",
      "Just say the word!\n"
     ]
    }
   ],
   "source": [
    "# First create the messages:\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"How can we explore the potential of LLMs in robotics?\"}]\n",
    "\n",
    "# Then make the first call:\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Then read the business idea:\n",
    "\n",
    "business_idea = response.choices[0].message.content\n",
    "\n",
    "# And repeat! In the next message, include the business idea within the message\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": business_idea}]\n",
    "\n",
    "# Then make the second call:\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Then read the pain point:\n",
    "\n",
    "pain_point = response.choices[0].message.content\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": f\"Propose a solution to the following pain point: {pain_point}\"}]\n",
    "\n",
    "# Then make the second call:\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Then read the pain point:\n",
    "\n",
    "solution = response.choices[0].message.content\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Exploring the potential of large language models (LLMs) in robotics is a promising area that combines advances in natural language understanding with physical interaction capabilities. Here are some key approaches and ideas to explore their potential:\n",
       "\n",
       "### 1. **Natural Language Interfaces for Robots**\n",
       "- **Goal:** Enable robots to understand and execute complex instructions given in natural language.\n",
       "- **Approach:** Use LLMs to parse commands, generate actionable plans, and translate high-level goals into low-level robotic actions.\n",
       "- **Example:** A user says, “Pick up the red cup from the table and put it in the sink,” and the robot leverages an LLM to interpret the command contextually.\n",
       "\n",
       "### 2. **Task Planning and Reasoning**\n",
       "- **Goal:** Use LLMs as planners or reasoning agents to structure multi-step tasks.\n",
       "- **Approach:** Combine LLM-generated sequences with robotic motion planning to handle abstract tasks by decomposing them into executable steps.\n",
       "- **Example:** An LLM breaks down “Clean the kitchen” into sub-tasks like “wash dishes,” “wipe counters,” and “take out trash,” which the robot then executes.\n",
       "\n",
       "### 3. **Learning from Language and Interaction**\n",
       "- **Goal:** Improve robots’ learning by leveraging rich textual knowledge and interactive feedback.\n",
       "- **Approach:** Use LLMs to provide prior knowledge about objects, environments, or social norms that can augment sensor-based learning.\n",
       "- **Example:** A robot asking clarifying questions or receiving corrections in natural language and updating its model accordingly.\n",
       "\n",
       "### 4. **Multi-modal Fusion**\n",
       "- **Goal:** Combine visual, tactile, and proprioceptive data with language understanding.\n",
       "- **Approach:** Integrate LLMs with visual models (like vision transformers) to ground language in perception and action.\n",
       "- **Example:** A robot understands “grab the cube next to the blue ball” by associating linguistic terms with visual input.\n",
       "\n",
       "### 5. **Sim-to-Real Transfer with Language Supervision**\n",
       "- **Goal:** Use LLMs to generate diverse language instructions and scenarios to train robotic policies in simulation.\n",
       "- **Approach:** Generate varied, naturalistic instructions to improve robustness and generalizability before deployment in the real world.\n",
       "\n",
       "### 6. **Safety and Explainability**\n",
       "- **Goal:** Use LLMs to explain robotic decision-making or to monitor for unsafe instructions.\n",
       "- **Approach:** Generate human-readable explanations for robot actions or flag ambiguous commands.\n",
       "\n",
       "### Practical Steps to Explore:\n",
       "- **Prototype integrations:** Start by integrating LLM APIs with robotic control frameworks (e.g., ROS).\n",
       "- **Benchmark tasks:** Design tasks requiring natural language understanding to measure performance gains.\n",
       "- **Data collection:** Collect datasets of paired language commands and robot actions.\n",
       "- **Iterative testing:** Evaluate how well LLM-guided instructions perform in real or simulated environments.\n",
       "- **Collaborations:** Work with NLP researchers to adapt LLMs for grounded language understanding.\n",
       "\n",
       "### Challenges to Consider:\n",
       "- Grounding LLMs in the real-world environment.\n",
       "- Handling ambiguous or incomplete instructions.\n",
       "- Real-time processing constraints.\n",
       "- Safety and robustness in physical interactions.\n",
       "\n",
       "---\n",
       "\n",
       "If you want, I can provide example code snippets or recommend specific frameworks and architectures to start experimenting!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(business_idea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a great overview of leveraging LLMs in robotics! If you want, I can help you with concrete next steps including example code snippets, recommended frameworks, or pointers to relevant models and datasets to jumpstart your experiments.\n",
       "\n",
       "### Example: Integrating an LLM for Command Parsing in a Robot\n",
       "\n",
       "Here's a high-level Python example illustrating how you might use an LLM (e.g., OpenAI GPT) to parse natural language commands and convert them into low-level robotic actions:\n",
       "\n",
       "```python\n",
       "import openai\n",
       "\n",
       "openai.api_key = \"YOUR_API_KEY\"\n",
       "\n",
       "def parse_command(command_text):\n",
       "    prompt = f\"\"\"\n",
       "    You are an assistant that converts natural language robot commands into structured steps.\n",
       "    Input command: \"{command_text}\"\n",
       "    Output steps as JSON list of simple actions with parameters, e.g.:\n",
       "    [\n",
       "        {{\"action\": \"move\", \"object\": \"red cup\", \"from\": \"table\"}},\n",
       "        {{\"action\": \"place\", \"object\": \"red cup\", \"location\": \"sink\"}}\n",
       "    ]\n",
       "    \"\"\"\n",
       "    response = openai.ChatCompletion.create(\n",
       "        model=\"gpt-4\",\n",
       "        messages=[{\"role\": \"system\", \"content\": \"You convert commands to robot action plans.\"},\n",
       "                  {\"role\": \"user\", \"content\": prompt}],\n",
       "        temperature=0,\n",
       "        max_tokens=200,\n",
       "    )\n",
       "    steps = response['choices'][0]['message']['content']\n",
       "    return steps\n",
       "\n",
       "command = \"Pick up the red cup from the table and put it in the sink.\"\n",
       "plan = parse_command(command)\n",
       "print(\"Planned steps for robot:\", plan)\n",
       "```\n",
       "\n",
       "### Why this helps:\n",
       "- The LLM acts as a semantic parser from language → structured plan.\n",
       "- Your robot controller can consume the structured output to trigger motions and manipulations.\n",
       "\n",
       "---\n",
       "\n",
       "### Frameworks & Tools to Consider\n",
       "\n",
       "- **Robot Operating System (ROS)**: Standard middleware for robot control; provides nodes & message passing.\n",
       "- **LangChain / LLM Chains**: Useful for orchestrating multi-step LLM calls and integrating external tools.\n",
       "- **CLIP / BLIP / Visual Transformers (ViT)**: To connect language with vision perception for grounding.\n",
       "- **Action Execution Simulator (e.g. Isaac Gym, PyBullet, or Habitat)**: To first test LLM-based command planning without hardware risks.\n",
       "- **RoboGPT / SayCan (from Google Research)**: Open-source projects combining LLMs + robot manipulation.\n",
       "\n",
       "---\n",
       "\n",
       "### Next Steps\n",
       "\n",
       "1. **Start small**: Build a simple language interface that turns commands into sequences of primitive robot actions.\n",
       "2. **Test in simulation**: Connect the textual plan output with a simulator to verify action feasibility.\n",
       "3. **Add grounding**: Integrate with perception modules to understand references to real-world objects.\n",
       "4. **Incorporate feedback**: Have the robot confirm commands or ask clarifying questions if uncertain.\n",
       "5. **Explore datasets**: Look at ALFRED, TEACh, or RoboNLP datasets which pair language with robotics tasks.\n",
       "\n",
       "---\n",
       "\n",
       "If you'd like sample code on fusing vision + language, multi-step planning with LLMs, or safety monitoring, just let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(pain_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is an excellent and practical approach to bridging LLMs with robotics! To build on your overview and proposed example, here’s a concrete solution to streamline and enhance the workflow of leveraging an LLM for command parsing and robotic action execution:\n",
       "\n",
       "---\n",
       "\n",
       "## Proposed Solution: Modular LLM-Powered Robotics Command Pipeline\n",
       "\n",
       "### Key Idea\n",
       "\n",
       "Create a modular software pipeline integrating natural language command parsing (using an LLM), grounding in perception modules, and robot control, with clear interfaces and fallback/feedback loops.\n",
       "\n",
       "---\n",
       "\n",
       "### Architecture Overview\n",
       "\n",
       "```\n",
       "User Command (NL)  -->  LLM Parser  -->  Structured Action Plan (JSON)  -->  \n",
       "     Perception Module (Vision, Localization) --> Plan Refinement & Validation -->  \n",
       "     Robot Control Interface  -->  Robot Execution  \n",
       "         ^                                                         |  \n",
       "         |------------------------------------ Feedback / Clarification Loop  \n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Detailed Pipeline Steps & Enhancements\n",
       "\n",
       "| Step                                      | Enhancement                                                 | Tools / Tips                              |\n",
       "|-------------------------------------------|-------------------------------------------------------------|------------------------------------------|\n",
       "| **1. Language Parsing (Your example)**    | - Use **few-shot prompting** or fine-tune an LLM on robotics task commands;<br>- Probabilistic parsing to output confidence scores | OpenAI GPT API / Local LLM (Llama, etc.) |\n",
       "| **2. Perception & Grounding**              | - Use vision models (e.g., CLIP or BLIP) to identify referenced objects in the scene <br>- Map language references (“red cup”) to detected objects with IDs | OpenCV, PyTorch + pre-trained vision models, ROS perception stack        |\n",
       "| **3. Plan Refinement & Validation**        | - Check whether the planned actions are feasible in the current environment<br>- Validate object locations, accessibility, and motion constraints | Use physics simulator (PyBullet, Isaac Gym), implement collision checks  |\n",
       "| **4. Execution Interface (ROS integration)**| - Translate JSON action steps into appropriate ROS messages or robot-specific API calls                        | ROS action servers, MoveIt! for motion planning                          |\n",
       "| **5. Feedback Loop**                        | - Robot confirms plan or requests clarification<br>- Integrate multi-turn dialogues with the LLM                     | LangChain for multi-step interactions, Dialogflow, or custom logic       |\n",
       "\n",
       "---\n",
       "\n",
       "### Sample Extended Python Snippet Sketch\n",
       "\n",
       "Here’s a sketch that extends your example with feedback and grounding placeholders:\n",
       "\n",
       "```python\n",
       "import openai\n",
       "\n",
       "openai.api_key = \"YOUR_API_KEY\"\n",
       "\n",
       "def parse_command(command_text):\n",
       "    prompt = f\"\"\"\n",
       "    You are an assistant that converts natural language robot commands into structured steps.\n",
       "    Input command: \"{command_text}\"\n",
       "    Output steps as JSON list of simple actions with parameters, e.g.:\n",
       "    [\n",
       "        {{\"action\": \"move\", \"object\": \"red cup\", \"from\": \"table\"}},\n",
       "        {{\"action\": \"place\", \"object\": \"red cup\", \"location\": \"sink\"}}\n",
       "    ]\n",
       "    \"\"\"\n",
       "    response = openai.ChatCompletion.create(\n",
       "        model=\"gpt-4\",\n",
       "        messages=[\n",
       "            {\"role\": \"system\", \"content\": \"You convert commands to robot action plans.\"},\n",
       "            {\"role\": \"user\", \"content\": prompt}\n",
       "        ],\n",
       "        temperature=0,\n",
       "        max_tokens=300,\n",
       "    )\n",
       "    steps_json = response['choices'][0]['message']['content']\n",
       "    return steps_json\n",
       "\n",
       "def ground_objects(steps, perception_data):\n",
       "    # Example: Map \"red cup\" -> detected object ID from perception\n",
       "    grounded_steps = []\n",
       "    for step in steps:\n",
       "        obj_name = step.get(\"object\", None)\n",
       "        if obj_name and obj_name in perception_data:\n",
       "            step[\"object_id\"] = perception_data[obj_name][\"id\"]\n",
       "        grounded_steps.append(step)\n",
       "    return grounded_steps\n",
       "\n",
       "def validate_plan(steps):\n",
       "    # Placeholder for plan plausibility checks (collision, reachability)\n",
       "    # Return True if valid, else False\n",
       "    return True\n",
       "\n",
       "def execute_plan(steps, robot_interface):\n",
       "    for step in steps:\n",
       "        # Convert step to robot commands\n",
       "        robot_interface.send_command(step)\n",
       "\n",
       "def main(command_text):\n",
       "    raw_plan = parse_command(command_text)\n",
       "    print(\"Raw plan:\", raw_plan)\n",
       "\n",
       "    # Convert raw_plan string to JSON (handle errors!)\n",
       "    import json\n",
       "    try:\n",
       "        steps = json.loads(raw_plan)\n",
       "    except json.JSONDecodeError:\n",
       "        print(\"Failed to parse plan JSON.\")\n",
       "        return\n",
       "\n",
       "    # Perception data mockup\n",
       "    perception_data = {\n",
       "        \"red cup\": {\"id\": 101, \"location\": \"table\"},\n",
       "        \"sink\": {\"id\": 202, \"location\": \"sink_area\"}\n",
       "    }\n",
       "\n",
       "    grounded_steps = ground_objects(steps, perception_data)\n",
       "    if not validate_plan(grounded_steps):\n",
       "        print(\"Plan validation failed: plan not executable.\")\n",
       "        return\n",
       "\n",
       "    # Mock robot interface\n",
       "    class RobotInterface:\n",
       "        def send_command(self, cmd):\n",
       "            print(f\"Executing: {cmd}\")\n",
       "\n",
       "    robot_interface = RobotInterface()\n",
       "    execute_plan(grounded_steps, robot_interface)\n",
       "    print(\"Plan executed successfully.\")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    user_command = \"Pick up the red cup from the table and put it in the sink.\"\n",
       "    main(user_command)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Summary\n",
       "\n",
       "- **LLM parses commands → structured plans**\n",
       "- **Perception grounds language references → real objects**\n",
       "- **Validation layer ensures feasibility**\n",
       "- **Robot interface executes validated plan**\n",
       "- **Feedback enables clarification and safety**\n",
       "\n",
       "This modular approach lets you gradually improve each stage while testing in simulation or on real hardware.\n",
       "\n",
       "---\n",
       "\n",
       "If you want, I can also share:\n",
       "\n",
       "- Sample ROS node templates bridging LLM output with robot motion planning\n",
       "- Vision-language grounding demo codes\n",
       "- Multi-turn clarification dialogs with LangChain\n",
       "- Dataset preprocessing scripts for ALFRED or TEACh\n",
       "\n",
       "Just say the word!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(solution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
